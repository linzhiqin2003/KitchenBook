"""
DashScope TTS Provider

Implements TTS using Alibaba's Qwen TTS models (qwen3-tts-flash, cosyvoice series).
"""

import os
import logging
import asyncio
from typing import AsyncIterator, List, Optional

import dashscope
from dashscope import MultiModalConversation

from .base import (
    TTSProvider,
    TTSResult,
    ProviderConfig,
    ServiceType,
)

logger = logging.getLogger(__name__)

# Available voices for Qwen TTS (qwen3-tts-flash)
# Full list from official documentation: https://help.aliyun.com/zh/model-studio/
QWEN_TTS_VOICES = {
    # Standard voices
    "Cherry": {"gender": "female", "style": "warm", "description": "èŠŠæ‚¦ - é˜³å…‰ç§¯æžã€äº²åˆ‡è‡ªç„¶çš„å¥³å£°", "emoji": "ðŸŒ¸"},
    "Ethan": {"gender": "male", "style": "warm", "description": "æ™¨ç…¦ - é˜³å…‰ã€æ¸©æš–ã€æ´»åŠ›çš„ç”·å£°", "emoji": "â˜€ï¸"},
    "Nofish": {"gender": "male", "style": "designer", "description": "ä¸åƒé±¼ - ä¸ä¼šç¿˜èˆŒéŸ³çš„è®¾è®¡å¸ˆ", "emoji": "ðŸŸ"},
    "Jennifer": {"gender": "female", "style": "professional", "description": "è©¹å¦®å¼— - å“ç‰Œçº§ç”µå½±è´¨æ„Ÿç¾Žè¯­å¥³å£°", "emoji": "ðŸŽ¬"},
    "Ryan": {"gender": "male", "style": "dramatic", "description": "ç”œèŒ¶ - èŠ‚å¥æ„Ÿå¼ºã€æˆæ„Ÿç‚¸è£‚", "emoji": "ðŸŽ­"},
    "Katerina": {"gender": "female", "style": "mature", "description": "å¡æ·ç³å¨œ - å¾¡å§éŸ³ã€éŸµå¾‹å›žå‘³åè¶³", "emoji": "ðŸ‘‘"},
    "Elias": {"gender": "male", "style": "lecturer", "description": "å¢¨è®²å¸ˆ - çŸ¥è¯†è®²è§£ã€å­¦æœ¯ä¸¥è°¨", "emoji": "ðŸ“š"},
    
    # Regional/Dialect voices
    "Jada": {"gender": "female", "style": "shanghai", "description": "ä¸Šæµ·é˜¿ç - é£Žé£Žç«ç«çš„æ²ªä¸Šé˜¿å§", "emoji": "ðŸ™ï¸"},
    "Dylan": {"gender": "male", "style": "beijing", "description": "åŒ—äº¬æ™“ä¸œ - åŒ—äº¬èƒ¡åŒå°‘å¹´", "emoji": "ðŸ¯"},
    "Sunny": {"gender": "female", "style": "sichuan", "description": "å››å·æ™´å„¿ - ç”œç¾Žå¯çˆ±çš„å·å¦¹å­", "emoji": "ðŸŒ¶ï¸"},
    "Li": {"gender": "male", "style": "nanjing", "description": "å—äº¬è€æŽ - è€å¿ƒçš„ç‘œä¼½è€å¸ˆ", "emoji": "ðŸ§˜"},
    "Marcus": {"gender": "male", "style": "shaanxi", "description": "é™•è¥¿ç§¦å· - å¯Œæœ‰è€é™•å‘³é“", "emoji": "â›°ï¸"},
    "Roy": {"gender": "male", "style": "minnan", "description": "é—½å—é˜¿æ° - è¯™è°ç›´çˆ½ã€å¸‚äº•æ´»æ³¼", "emoji": "ðŸµ"},
}

# Language mapping
LANGUAGE_MAPPING = {
    "Chinese": "Chinese",
    "zh": "Chinese",
    "cn": "Chinese",
    "ä¸­æ–‡": "Chinese",
    "English": "English",
    "en": "English",
    "è‹±æ–‡": "English",
    "Japanese": "Japanese",
    "jp": "Japanese",
    "ja": "Japanese",
    "æ—¥è¯­": "Japanese",
    "Korean": "Korean",
    "ko": "Korean",
    "éŸ©è¯­": "Korean",
    "French": "French",
    "fr": "French",
    "æ³•è¯­": "French",
    "German": "German",
    "de": "German",
    "å¾·è¯­": "German",
    "Spanish": "Spanish",
    "es": "Spanish",
    "è¥¿ç­ç‰™è¯­": "Spanish",
    "Portuguese": "Portuguese",
    "pt": "Portuguese",
    "è‘¡è„ç‰™è¯­": "Portuguese",
    "Italian": "Italian",
    "it": "Italian",
    "æ„å¤§åˆ©è¯­": "Italian",
    "Russian": "Russian",
    "ru": "Russian",
    "ä¿„è¯­": "Russian",
    "Auto": "Auto",
    "auto": "Auto",
}


class TTSResponse:
    """Response wrapper for TTS API"""
    def __init__(self, audio_url: str = None, audio_data: bytes = None, 
                 characters: int = 0, expires_at: int = None, request_id: str = None):
        self.audio_url = audio_url
        self.audio_data = audio_data
        self.characters = characters
        self.expires_at = expires_at
        self.request_id = request_id


class DashScopeTTSProvider(TTSProvider):
    """
    DashScope TTS Provider using Qwen TTS models.
    
    Supports:
    - qwen3-tts-flash (recommended, faster, up to 600 chars)
    - cosyvoice-v2 (higher quality)
    """
    
    DEFAULT_MODEL = "qwen3-tts-flash"
    DEFAULT_VOICE = "Cherry"
    
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        
        # Set DashScope API key
        self.api_key = config.api_key or os.environ.get("DASHSCOPE_API_KEY")
        if not self.api_key:
            raise ValueError("DASHSCOPE_API_KEY is required")
        
        # Set base URL for DashScope
        dashscope.base_http_api_url = 'https://dashscope.aliyuncs.com/api/v1'
        
        self.model = config.model or self.DEFAULT_MODEL
        self.voice = config.extra_params.get("voice", self.DEFAULT_VOICE)
    
    def get_supported_services(self) -> List[ServiceType]:
        return [ServiceType.TTS]
    
    def synthesize(
        self,
        text: str,
        voice: Optional[str] = None,
        language: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> TTSResult:
        """
        Synthesize text to speech (synchronous).
        
        Args:
            text: Text to synthesize (max 600 chars for qwen3-tts-flash)
            voice: Voice ID (default: Cherry)
            language: Language hint (default: Auto)
            model: Model to use (default: qwen3-tts-flash)
            
        Returns:
            TTSResult with audio_data (bytes) containing the synthesized audio
        """
        voice = voice or self.voice
        language = LANGUAGE_MAPPING.get(language, "Auto") if language else "Auto"
        model = model or self.model
        
        try:
            response = MultiModalConversation.call(
                model=model,
                api_key=self.api_key,
                text=text,
                voice=voice,
                language_type=language,
            )
            
            if response.status_code != 200:
                raise Exception(f"TTS API error: {response.code} - {response.message}")
            
            # Get audio URL from response
            audio_url = response.output.audio.get("url", "")
            
            if not audio_url:
                raise Exception("No audio URL in response")
            
            # For synchronous call, we return a TTSResult with empty audio_data
            # but include the URL in format field for the caller to download
            return TTSResult(
                audio_data=b"",  # Empty, use the URL instead
                format=audio_url,  # Store URL in format field
                sample_rate=16000,
            )
            
        except Exception as e:
            self.logger.error(f"TTS synthesis error: {e}")
            raise
    
    async def synthesize_async(
        self,
        text: str,
        voice: Optional[str] = None,
        language: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> TTSResponse:
        """
        Synthesize text to speech (asynchronous).
        
        Returns TTSResponse with audio_url for the caller to use.
        """
        voice = voice or self.voice
        language = LANGUAGE_MAPPING.get(language, "Auto") if language else "Auto"
        model = model or self.model
        
        try:
            # Run in thread pool since dashscope SDK is synchronous
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: MultiModalConversation.call(
                    model=model,
                    api_key=self.api_key,
                    text=text,
                    voice=voice,
                    language_type=language,
                )
            )
            
            if response.status_code != 200:
                raise Exception(f"TTS API error: {response.code} - {response.message}")
            
            audio_info = response.output.audio
            
            return TTSResponse(
                audio_url=audio_info.get("url", ""),
                characters=response.usage.get("characters", 0) if response.usage else 0,
                expires_at=audio_info.get("expires_at"),
                request_id=response.request_id,
            )
            
        except Exception as e:
            self.logger.error(f"Async TTS synthesis error: {e}")
            raise
    
    async def synthesize_stream(
        self,
        text: str,
        voice: Optional[str] = None,
        **kwargs
    ) -> AsyncIterator[bytes]:
        """
        Streaming text-to-speech synthesis.
        
        Note: This is a simplified implementation that yields the complete audio.
        For true streaming, you would need to use the DashScope streaming API.
        """
        voice = voice or self.voice
        language = kwargs.get("language", "Auto")
        
        try:
            # For now, use non-streaming and yield the result
            response = await self.synthesize_async(text, voice, language)
            
            if response.audio_url:
                # Yield the URL as a marker (caller should handle downloading)
                yield response.audio_url.encode('utf-8')
            
        except Exception as e:
            self.logger.error(f"Stream TTS error: {e}")
            raise
    
    @staticmethod
    def get_available_voices() -> dict:
        """Get available voice options"""
        return QWEN_TTS_VOICES
    
    @staticmethod
    def get_supported_languages() -> list:
        """Get supported languages"""
        return list(set(LANGUAGE_MAPPING.values()))
