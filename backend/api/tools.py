"""
AI Lab å·¥å…·æ³¨å†Œè¡¨ - ä¾› agentic loop ä½¿ç”¨

å¢å¼ºç‰ˆ web_searchï¼š
- Google CSE + Serper åŒå¼•æ“æœç´¢
- Jina Reader ç½‘é¡µæŠ“å–
- Cerebras AI ç»“æ„åŒ–æå–ï¼ˆDeepSeek é™çº§ï¼‰
- Consolidation ç»¼åˆæŠ¥å‘Š + å¼•ç”¨æ ‡è®° [REF:n]
"""
import json
import logging
import re
import math
import urllib.request
import urllib.error
import urllib.parse
from datetime import datetime
from django.conf import settings

logger = logging.getLogger(__name__)


# ==================== å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰ ====================

TOOL_DEFINITIONS = [
    {
        "type": "function",
        "function": {
            "name": "get_current_datetime",
            "description": "è·å–å½“å‰çš„æ—¥æœŸå’Œæ—¶é—´ï¼ŒåŒ…æ‹¬æ˜ŸæœŸå‡ ã€‚å½“ç”¨æˆ·è¯¢é—®ç°åœ¨å‡ ç‚¹ã€ä»Šå¤©å‡ å·ã€æ˜ŸæœŸå‡ ç­‰æ—¶é—´ç›¸å…³é—®é¢˜æ—¶ä½¿ç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculator",
            "description": "å®‰å…¨è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚æ”¯æŒåŠ å‡ä¹˜é™¤ã€å¹‚è¿ç®—ã€æ‹¬å·ã€å¸¸ç”¨æ•°å­¦å‡½æ•°ï¼ˆsin, cos, sqrt, log ç­‰ï¼‰ã€‚å½“ç”¨æˆ·éœ€è¦æ•°å­¦è®¡ç®—æ—¶ä½¿ç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ '123 * 456'ã€'sqrt(144)'ã€'2 ** 10'"
                    }
                },
                "required": ["expression"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "æœç´¢äº’è”ç½‘è·å–å®æ—¶ä¿¡æ¯ã€‚è‡ªåŠ¨æŠ“å–ç½‘é¡µå¹¶ç”Ÿæˆ AI æ‘˜è¦ï¼Œç»“æœåŒ…å«å¼•ç”¨æ ‡è®° [REF:n]ã€‚å½“ç”¨æˆ·è¯¢é—®æœ€æ–°æ–°é—»ã€å®æ—¶æ•°æ®ã€ä¸ç¡®å®šçš„äº‹å®ã€æˆ–ä»»ä½•éœ€è¦è”ç½‘æŸ¥è¯¢çš„é—®é¢˜æ—¶ä½¿ç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "search_type": {
                        "type": "string",
                        "enum": ["search", "news"],
                        "description": "æœç´¢ç±»å‹ï¼š'search' ä¸ºå¸¸è§„ç½‘é¡µæœç´¢ï¼Œ'news' ä¸ºæ–°é—»æœç´¢ã€‚é»˜è®¤ 'search'"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡ï¼Œ1-10ï¼Œé»˜è®¤ 5"
                    }
                },
                "required": ["query"]
            }
        }
    },
]


# ==================== å·¥å…·å¤„ç†å‡½æ•° ====================

def handle_get_current_datetime(**kwargs):
    """è¿”å›å½“å‰æ—¥æœŸæ—¶é—´å’Œæ˜ŸæœŸå‡ """
    now = datetime.now()
    weekdays = ['æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥']
    return (
        f"å½“å‰æ—¶é—´ï¼š{now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')} "
        f"{weekdays[now.weekday()]}"
    )


# è®¡ç®—å™¨å…è®¸çš„å®‰å…¨åç§°
_CALC_SAFE_NAMES = {
    "abs": abs, "round": round, "min": min, "max": max,
    "sin": math.sin, "cos": math.cos, "tan": math.tan,
    "asin": math.asin, "acos": math.acos, "atan": math.atan,
    "sqrt": math.sqrt, "log": math.log, "log2": math.log2, "log10": math.log10,
    "exp": math.exp, "pow": pow, "ceil": math.ceil, "floor": math.floor,
    "pi": math.pi, "e": math.e,
}

# ç™½åå•å­—ç¬¦
_CALC_ALLOWED_CHARS = re.compile(r'^[0-9+\-*/().,%^ \t\n\w]+$')


def handle_calculator(expression="", **kwargs):
    """å®‰å…¨è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    expression = expression.strip()
    if not expression:
        raise ValueError("è¡¨è¾¾å¼ä¸èƒ½ä¸ºç©º")

    if not _CALC_ALLOWED_CHARS.match(expression):
        raise ValueError(f"è¡¨è¾¾å¼åŒ…å«ä¸å…è®¸çš„å­—ç¬¦: {expression}")

    # å°† ^ æ›¿æ¢ä¸º **ï¼ˆå¹‚è¿ç®—ï¼‰
    expression = expression.replace('^', '**')

    try:
        result = eval(expression, {"__builtins__": {}}, _CALC_SAFE_NAMES)  # noqa: S307
    except Exception as exc:
        raise ValueError(f"è®¡ç®—å¤±è´¥: {exc}") from exc

    return f"{expression.replace('**', '^')} = {result}"


# ==================== å¢å¼ºç‰ˆ Web Search ====================

# éœ€è¦å»é™¤çš„ tracking å‚æ•°
_TRACKING_PARAMS = {
    'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
    'gclid', 'fbclid', 'ref', 'source', 'spm', 'from', 'isappinstalled',
}

# Jina æŠ“å–å†…å®¹æœ€å¤§å­—ç¬¦æ•°
_JINA_MAX_CHARS = 8000

# æŠ“å– top N URL
_SCRAPE_TOP_N = 3

# Cerebras API é…ç½®
_CEREBRAS_BASE_URL = "https://api.cerebras.ai/v1"
_CEREBRAS_MODEL = "gpt-oss-120b"

# Cerebras key pool round-robin è®¡æ•°å™¨
_cerebras_key_index = 0


def _normalize_url(url):
    """æ¸…ç† URLï¼šå»é™¤ tracking å‚æ•°å’Œ fragment"""
    try:
        parsed = urllib.parse.urlparse(url)
        # å»é™¤ fragment
        parsed = parsed._replace(fragment='')
        # è¿‡æ»¤ tracking å‚æ•°
        qs = urllib.parse.parse_qs(parsed.query, keep_blank_values=False)
        cleaned_qs = {
            k: v for k, v in qs.items()
            if k.lower() not in _TRACKING_PARAMS
        }
        new_query = urllib.parse.urlencode(cleaned_qs, doseq=True)
        parsed = parsed._replace(query=new_query)
        return urllib.parse.urlunparse(parsed)
    except Exception:
        return url


def _extract_domain(url):
    """ä» URL æå–åŸŸå"""
    try:
        return urllib.parse.urlparse(url).netloc
    except Exception:
        return url


def _search_google_cse(query, num=5):
    """Google Custom Search Engine æœç´¢"""
    api_key = getattr(settings, 'GOOGLE_CSE_API_KEY', '')
    cx = getattr(settings, 'GOOGLE_CSE_CX', '')
    if not api_key or not cx:
        return None

    params = urllib.parse.urlencode({
        'key': api_key,
        'cx': cx,
        'q': query,
        'num': min(num, 10),
    })
    url = f"https://www.googleapis.com/customsearch/v1?{params}"
    req = urllib.request.Request(url, method="GET")

    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read().decode("utf-8"))
    except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:
        logger.warning("Google CSE search failed: %s", e)
        return None

    items = data.get("items", [])
    if not items:
        return None

    results = []
    for item in items:
        results.append({
            "title": item.get("title", ""),
            "snippet": item.get("snippet", ""),
            "link": item.get("link", ""),
        })
    return results


def _search_serper(query, search_type="search", num=5):
    """Serper API æœç´¢ï¼ˆå¤‡ç”¨å¼•æ“ï¼‰"""
    api_key = getattr(settings, 'SERPER_API_KEY', '')
    if not api_key:
        return None

    endpoint = "news" if search_type == "news" else "search"
    url = f"https://google.serper.dev/{endpoint}"
    payload = json.dumps({"q": query, "num": num}).encode("utf-8")
    req = urllib.request.Request(
        url,
        data=payload,
        headers={
            "X-API-KEY": api_key,
            "Content-Type": "application/json",
        },
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read().decode("utf-8"))
    except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:
        logger.warning("Serper search failed: %s", e)
        return None

    raw_items = data.get("news" if search_type == "news" else "organic", [])
    if not raw_items:
        return None

    results = []
    for item in raw_items:
        results.append({
            "title": item.get("title", ""),
            "snippet": item.get("snippet", ""),
            "link": item.get("link", ""),
            "date": item.get("date", ""),
        })
    return results


def _jina_scrape(url, timeout=15):
    """ä½¿ç”¨ Jina Reader æŠ“å–ç½‘é¡µå†…å®¹ï¼ˆçº¯æ–‡æœ¬ï¼‰"""
    jina_url = f"https://r.jina.ai/{url}"
    req = urllib.request.Request(
        jina_url,
        headers={"Accept": "text/plain"},
        method="GET",
    )
    try:
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            content = resp.read().decode("utf-8", errors="replace").strip()
            # æˆªæ–­åˆ°æœ€å¤§å­—ç¬¦æ•°
            if len(content) > _JINA_MAX_CHARS:
                content = content[:_JINA_MAX_CHARS] + "\n...(å†…å®¹å·²æˆªæ–­)"
            return content
    except Exception as e:
        logger.warning("Jina scrape failed for %s: %s", url, e)
        return None


def _get_cerebras_key():
    """ä» key pool ä¸­è½®è¯¢å–ä¸€ä¸ª Cerebras API key"""
    global _cerebras_key_index
    pool_str = getattr(settings, 'CEREBRAS_API_KEY_POOL', '')
    if not pool_str:
        return None
    keys = [k.strip() for k in pool_str.split(',') if k.strip()]
    if not keys:
        return None
    key = keys[_cerebras_key_index % len(keys)]
    _cerebras_key_index += 1
    return key


def _llm_call(system_msg, user_msg, max_tokens=2048, timeout=30):
    """é€šç”¨ LLM è°ƒç”¨ï¼šCerebras ä¼˜å…ˆ â†’ DeepSeek é™çº§ã€‚è¿”å›æ–‡æœ¬æˆ– Noneã€‚"""
    # â”€â”€ å°è¯• Cerebrasï¼ˆæé€Ÿæ¨ç†ï¼‰â”€â”€
    cerebras_key = _get_cerebras_key()
    if cerebras_key:
        payload = json.dumps({
            "model": _CEREBRAS_MODEL,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            "max_tokens": max_tokens,
            "temperature": 0.1,
        }).encode("utf-8")
        req = urllib.request.Request(
            f"{_CEREBRAS_BASE_URL}/chat/completions",
            data=payload,
            headers={
                "Authorization": f"Bearer {cerebras_key}",
                "Content-Type": "application/json",
            },
            method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                data = json.loads(resp.read().decode("utf-8"))
            text = data["choices"][0]["message"]["content"].strip()
            if text:
                return text
        except Exception as e:
            logger.warning("Cerebras LLM call failed, falling back to DeepSeek: %s", e)

    # â”€â”€ é™çº§åˆ° DeepSeek â”€â”€
    api_key = getattr(settings, 'DEEPSEEK_API_KEY', '')
    base_url = getattr(settings, 'DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
    if not api_key:
        return None

    payload = json.dumps({
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        "max_tokens": max_tokens,
        "temperature": 0.3,
    }).encode("utf-8")
    req = urllib.request.Request(
        f"{base_url}/chat/completions",
        data=payload,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        },
        method="POST",
    )
    try:
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            data = json.loads(resp.read().decode("utf-8"))
        return data["choices"][0]["message"]["content"].strip()
    except Exception as e:
        logger.warning("DeepSeek LLM call failed: %s", e)
        return None


# â”€â”€ ç»“æ„åŒ–æå– promptï¼ˆç§»æ¤è‡ª research_agentï¼‰ â”€â”€

_EXTRACT_SYSTEM_MSG = (
    "ä½ æ˜¯ä¸¥è°¨çš„ç ”ç©¶åŠ©æ‰‹ï¼Œä¸“æ³¨äºä»ç½‘é¡µä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚"
    "ä½ çš„è¾“å‡ºå°†ç›´æ¥ä¾›ä¸‹æ¸¸åˆ†æä½¿ç”¨ï¼Œå› æ­¤å¿…é¡»ï¼š\n"
    "1) ä¸¥æ ¼éµå¾ªæŒ‡å®šçš„ Markdown è¾“å‡ºæ ¼å¼\n"
    "2) ä¿ç•™æ‰€æœ‰é‡åŒ–æ•°æ®ï¼ˆæ•°å­—ã€ç™¾åˆ†æ¯”ã€é‡‘é¢ï¼‰åŠå…¶ä¸Šä¸‹æ–‡\n"
    "3) ä¿æŒä¿¡æ¯å¯†åº¦ï¼Œä¸è¦æ³¨æ°´æˆ–é‡å¤\n"
    "4) å¦‚æœä¿¡æ¯ä¸ç¡®å®šï¼Œæ˜ç¡®æ ‡æ³¨ [å¾…æ ¸å®]\n"
    "5) ä½¿ç”¨ä¸­æ–‡è¾“å‡º"
)

_EXTRACT_PROMPT_TEMPLATE = (
    "åŸºäºä»¥ä¸‹ç½‘é¡µæ­£æ–‡ï¼Œæå–ç»“æ„åŒ–ç ”ç©¶æ‘˜è¦ã€‚\n\n"
    "æ¥æºæ ‡è®°ï¼š[REF:{ref_id}]\n\n"
    "ã€ä¸¥æ ¼è¦æ±‚ã€‘\n"
    "- åªåŸºäºæä¾›çš„æ–‡æœ¬ï¼Œç¦æ­¢ç¼–é€ ä»»ä½•ä¿¡æ¯\n"
    "- ä¿ç•™æ‰€æœ‰å…³é”®æ•°å­—ã€æ—¥æœŸã€ç™¾åˆ†æ¯”ã€é‡‘é¢ã€äººåã€å…¬å¸å\n"
    "- ä½¿ç”¨ä¸­æ–‡è¾“å‡º\n\n"
    "ã€è¾“å‡ºæ ¼å¼ã€‘æŒ‰ä»¥ä¸‹ Markdown ç»“æ„è¾“å‡ºï¼š\n\n"
    "### æ ¸å¿ƒå‘ç°\n"
    "(ç”¨ 1-3 å¥è¯æ¦‚æ‹¬æœ¬é¡µæœ€é‡è¦çš„ä¿¡æ¯ï¼Œæ ‡æ³¨ [REF:{ref_id}])\n\n"
    "### å…³é”®æ•°æ®\n"
    "| æŒ‡æ ‡ | æ•°å€¼ | æ—¶é—´/å£å¾„ | å˜åŠ¨ |\n"
    "| --- | --- | --- | --- |\n"
    "| (æŒ‡æ ‡å) | (å…·ä½“æ•°å€¼) | (è´¢å¹´/å­£åº¦/æ—¥æœŸ) | (åŒæ¯”/ç¯æ¯”å˜åŒ–) |\n\n"
    "(å¦‚é¡µé¢æ— é‡åŒ–æ•°æ®ï¼Œå†™ã€Œæœ¬é¡µæ— é‡åŒ–æ•°æ®ã€)\n\n"
    "### å®šæ€§è¦ç‚¹\n"
    "- (é‡è¦çš„éæ•°å€¼ä¿¡æ¯ï¼Œæ ‡æ³¨ [REF:{ref_id}])\n\n"
    "--- ç½‘é¡µæ­£æ–‡ ---\n"
    "æ ‡é¢˜ï¼š{title}\n"
    "URLï¼š{url}\n\n"
    "{content}"
)

# â”€â”€ Consolidation promptï¼ˆç§»æ¤è‡ª research_agentï¼‰ â”€â”€

_CONSOLIDATE_SYSTEM_MSG = (
    "ä½ æ˜¯ä¸¥è°¨çš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†å¤šä¸ªç½‘é¡µæ‘˜è¦ç»¼åˆä¸ºä¸€ä»½ç²¾ç‚¼çš„ç ”ç©¶æ‘˜è¦ã€‚\n"
    "è¦æ±‚ï¼š\n"
    "1) ä¿ç•™æ‰€æœ‰é‡åŒ–æ•°æ®ï¼ˆæ•°å­—ã€ç™¾åˆ†æ¯”ã€é‡‘é¢ï¼‰åŠå…¶ä¸Šä¸‹æ–‡\n"
    "2) å»é™¤ä¸åŒæ¥æºé—´çš„é‡å¤ä¿¡æ¯ï¼Œä¿ç•™æœ€è¯¦ç»†çš„ç‰ˆæœ¬\n"
    "3) æŒ‰ä¸»é¢˜ç»„ç»‡ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æŒ‰æ¥æºç½—åˆ—\n"
    "4) ä½¿ç”¨ã€å¼•ç”¨æ˜ å°„è¡¨ã€‘ä¸­æä¾›çš„å…¨å±€ [REF:n] ç¼–å·æ ‡æ³¨æ•°æ®æ¥æºï¼Œç¦æ­¢è‡ªè¡Œç¼–é€ å¼•ç”¨ç¼–å·\n"
    "5) ä½¿ç”¨ä¸­æ–‡è¾“å‡º\n"
    "6) åªåŸºäºæä¾›çš„å†…å®¹ï¼Œç¦æ­¢ç¼–é€ "
)

_CONSOLIDATE_PROMPT_TEMPLATE = (
    "ä»¥ä¸‹æ˜¯é’ˆå¯¹æœç´¢è¯ã€Œ{query}ã€ä»å¤šä¸ªç½‘é¡µæå–çš„æ‘˜è¦ã€‚"
    "è¯·å°†æ‰€æœ‰æ¥æºçš„ä¿¡æ¯ç»¼åˆä¸ºä¸€ä»½ç²¾ç‚¼çš„ç»“æ„åŒ–ç ”ç©¶æ‘˜è¦ã€‚\n\n"
    "ã€å¼•ç”¨æ˜ å°„è¡¨ã€‘\n"
    "ä»¥ä¸‹æ˜¯æœ¬æ¬¡æ¶‰åŠçš„é¡µé¢ URL åŠå…¶å…¨å±€å¼•ç”¨ IDï¼Œè¯·åœ¨è¾“å‡ºä¸­ä½¿ç”¨è¿™äº› [REF:n] ç¼–å·æ ‡æ³¨æ•°æ®æ¥æºï¼š\n"
    "{ref_mapping}\n\n"
    "ã€ä¸¥æ ¼è¦æ±‚ã€‘\n"
    "- åœ¨å…³é”®æ•°æ®è¡¨å’Œæ­£æ–‡ä¸­å¿…é¡»ä½¿ç”¨ä¸Šæ–¹ã€å¼•ç”¨æ˜ å°„è¡¨ã€‘æä¾›çš„ [REF:n] ç¼–å·æ ‡æ³¨æ•°æ®å‡ºå¤„\n"
    "- ç¦æ­¢è‡ªè¡Œç¼–é€ å¼•ç”¨ç¼–å·ï¼›å¦‚æŸæ•°æ®æ— æ³•å½’å±åˆ°å…·ä½“æ¥æºï¼Œæ³¨æ˜ã€Œæ¥æºä¸æ˜ã€\n\n"
    "ã€è¾“å‡ºæ ¼å¼ã€‘\n"
    "### æ ¸å¿ƒå‘ç°\n"
    "(3-5 æ¡æœ€é‡è¦çš„å‘ç°ï¼Œæ¯æ¡ä¸€è¡Œï¼Œæ ‡æ³¨ [REF:n])\n\n"
    "### å…³é”®æ•°æ®\n"
    "| æŒ‡æ ‡ | æ•°å€¼ | æ—¶é—´/å£å¾„ | å˜åŠ¨ | æ¥æº |\n"
    "| --- | --- | --- | --- | --- |\n"
    "(åˆå¹¶æ‰€æœ‰æ¥æºçš„é‡åŒ–æ•°æ®ï¼Œã€Œæ¥æºã€åˆ—ä½¿ç”¨ [REF:n] ç¼–å·)\n\n"
    "### è¯¦ç»†åˆ†æ\n"
    "(æŒ‰ä¸»é¢˜ç»„ç»‡çš„æ·±å…¥åˆ†æï¼Œå…³é”®æ•°æ®æ ‡æ³¨ [REF:n])\n\n"
    "### é£é™©ä¸ä¸ç¡®å®šæ€§\n"
    "(å¦‚æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ ‡æ³¨ [REF:n])\n\n"
    "--- å¾…ç»¼åˆå†…å®¹ ---\n"
    "{content}"
)


def _ai_extract(title, content, ref_id, url):
    """ä½¿ç”¨ Cerebras/DeepSeek å¯¹ç½‘é¡µå†…å®¹åšç»“æ„åŒ–æå–"""
    prompt = _EXTRACT_PROMPT_TEMPLATE.format(
        ref_id=ref_id,
        title=title,
        url=url,
        content=content[:6000],
    )
    return _llm_call(_EXTRACT_SYSTEM_MSG, prompt, max_tokens=2048, timeout=30)


def _ai_consolidate(summaries, references, query):
    """ä½¿ç”¨ Cerebras/DeepSeek å¯¹å¤šæ¡æ‘˜è¦åšç»¼åˆæŠ¥å‘Š"""
    # æ„å»ºå¼•ç”¨æ˜ å°„è¡¨
    ref_mapping = ""
    for ref_id, url, title, domain in references:
        ref_mapping += f"- [REF:{ref_id}] {url}\n"

    # æ„å»ºå¾…ç»¼åˆå†…å®¹
    combined = ""
    for s in summaries:
        combined += f"\n--- [REF:{s['ref_id']}] {s['title']} ---\n{s['content']}\n"

    prompt = _CONSOLIDATE_PROMPT_TEMPLATE.format(
        query=query,
        ref_mapping=ref_mapping,
        content=combined,
    )
    return _llm_call(_CONSOLIDATE_SYSTEM_MSG, prompt, max_tokens=4096, timeout=45)


def handle_web_search(query="", search_type="search", max_results=5, **kwargs):
    """å¢å¼ºç‰ˆ web_searchï¼šæœç´¢ + æŠ“å– + AI æ‘˜è¦ + å¼•ç”¨æ ‡è®°"""
    query = query.strip()
    if not query:
        raise ValueError("æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º")

    max_results = max(1, min(10, int(max_results)))

    # â”€â”€ 1. åŒå¼•æ“æœç´¢ï¼šGoogle CSE â†’ Serper â”€â”€
    search_results = None

    # ä¼˜å…ˆ Google CSEï¼ˆnews æ¨¡å¼ CSE ä¸æ”¯æŒï¼Œç›´æ¥èµ° Serperï¼‰
    if search_type != "news":
        search_results = _search_google_cse(query, num=max_results)
        if search_results:
            logger.info("Search via Google CSE: %d results", len(search_results))

    # CSE å¤±è´¥æˆ– news æ¨¡å¼ï¼Œèµ° Serper
    if not search_results:
        search_results = _search_serper(query, search_type=search_type, num=max_results)
        if search_results:
            logger.info("Search via Serper: %d results", len(search_results))

    if not search_results:
        raise ValueError("æœç´¢å¼•æ“å‡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ API é…ç½®")

    if not search_results:
        return f"æœªæ‰¾åˆ°ä¸ \"{query}\" ç›¸å…³çš„ç»“æœã€‚"

    # â”€â”€ 2. æ„å»ºå¼•ç”¨åˆ—è¡¨ & æ¸…ç† URL â”€â”€
    references = []  # [(ref_id, url, title, domain)]
    for i, item in enumerate(search_results[:max_results], 1):
        url = _normalize_url(item.get("link", ""))
        title = item.get("title", "æ— æ ‡é¢˜")
        domain = _extract_domain(url)
        references.append((i, url, title, domain))

    # â”€â”€ 3. Jina Reader æŠ“å– top N URL â”€â”€
    scraped = []  # [{ref_id, title, content, url}]
    scrape_status = {}  # ref_id â†’ 'scraped' | 'snippet' | 'skipped'
    for ref_id, url, title, domain in references[:_SCRAPE_TOP_N]:
        if not url:
            scrape_status[ref_id] = 'skipped'
            continue
        content = _jina_scrape(url)
        if content and len(content) > 100:
            scraped.append({
                "ref_id": ref_id,
                "title": title,
                "content": content,
                "url": url,
            })
            scrape_status[ref_id] = 'scraped'
        else:
            # Jina æŠ“å–å¤±è´¥ï¼Œç”¨æœç´¢ snippet æ›¿ä»£
            snippet = search_results[ref_id - 1].get("snippet", "")
            if snippet:
                scraped.append({
                    "ref_id": ref_id,
                    "title": title,
                    "content": snippet,
                    "url": url,
                })
            scrape_status[ref_id] = 'snippet'
    # æœªå°è¯•æŠ“å–çš„æ ‡è®°ä¸º skipped
    for ref_id, url, title, domain in references[_SCRAPE_TOP_N:]:
        scrape_status[ref_id] = 'skipped'

    # â”€â”€ 3.5 æ„å»ºæŠ“å–çŠ¶æ€å¤´éƒ¨ â”€â”€
    scraped_count = sum(1 for s in scrape_status.values() if s == 'scraped')
    status_lines = [f"ğŸ” æœç´¢ã€Œ{query}ã€| {len(references)} æ¡ç»“æœï¼Œå·²æŠ“å– {scraped_count} ä¸ªç½‘é¡µ\n"]
    for ref_id, url, title, domain in references:
        st = scrape_status.get(ref_id, 'skipped')
        if st == 'scraped':
            status_lines.append(f"  [REF:{ref_id}] {domain} âœ…")
        elif st == 'snippet':
            status_lines.append(f"  [REF:{ref_id}] {domain} âš ï¸ ä»…æ‘˜è¦")
        else:
            status_lines.append(f"  [REF:{ref_id}] {domain}")
    scrape_header = "\n".join(status_lines) + "\n\n---\n\n"

    # â”€â”€ 4. Cerebras/DeepSeek ç»“æ„åŒ–æå– â”€â”€
    summaries = []
    for item in scraped:
        extracted = _ai_extract(
            item["title"], item["content"], item["ref_id"], item["url"],
        )
        if extracted:
            summaries.append({
                "ref_id": item["ref_id"],
                "title": item["title"],
                "content": extracted,
            })
        else:
            # AI æå–å¤±è´¥ï¼Œç”¨æˆªæ–­çš„åŸæ–‡å†…å®¹æ›¿ä»£
            truncated = item["content"][:500]
            summaries.append({
                "ref_id": item["ref_id"],
                "title": item["title"],
                "content": truncated,
            })

    # â”€â”€ 5. Consolidationï¼ˆ2+ æ¡æ‘˜è¦ä¸”æ€»é•¿åº¦ > 500 å­—ï¼‰ â”€â”€
    total_summary_len = sum(len(s["content"]) for s in summaries)

    if len(summaries) >= 2 and total_summary_len > 500:
        consolidated = _ai_consolidate(summaries, references, query)
        if consolidated:
            return scrape_header + consolidated

    # â”€â”€ 6. é™çº§ï¼šç›´æ¥æ‹¼æ¥æ‘˜è¦ + å¼•ç”¨åˆ—è¡¨ â”€â”€
    lines = [scrape_header]

    # æœ‰æ‘˜è¦æ—¶è¾“å‡ºæ‘˜è¦
    if summaries:
        for s in summaries:
            lines.append(f"**[REF:{s['ref_id']}] {s['title']}**")
            lines.append(s["content"])
            lines.append("")
    else:
        # æ²¡æœ‰æ‘˜è¦ï¼Œè¾“å‡ºåŸå§‹æœç´¢ç»“æœ
        for i, item in enumerate(search_results[:max_results], 1):
            title = item.get("title", "æ— æ ‡é¢˜")
            snippet = item.get("snippet", "æ— æ‘˜è¦")
            link = item.get("link", "")
            date_str = item.get("date", "")
            lines.append(f"{i}. {title}")
            if date_str:
                lines.append(f"   æ—¥æœŸ: {date_str}")
            lines.append(f"   {snippet}")
            if link:
                lines.append(f"   é“¾æ¥: {link}")
            lines.append("")

    # é™„å¼•ç”¨åˆ—è¡¨
    lines.append("### å¼•ç”¨æ¥æº")
    for ref_id, url, title, domain in references:
        lines.append(f"[REF:{ref_id}] {title} - {url}")

    return "\n".join(lines)


# ==================== å·¥å…·æ˜ å°„ & ç»Ÿä¸€æ‰§è¡Œå…¥å£ ====================

TOOL_HANDLERS = {
    "get_current_datetime": handle_get_current_datetime,
    "calculator": handle_calculator,
    "web_search": handle_web_search,
}


def execute_tool(name, args):
    """ç»Ÿä¸€æ‰§è¡Œå…¥å£ï¼Œè¿”å› (result_str, error_str)"""
    handler = TOOL_HANDLERS.get(name)
    if not handler:
        return None, f"æœªçŸ¥å·¥å…·: {name}"
    try:
        result = handler(**args)
        return str(result), None
    except Exception as exc:
        return None, str(exc)
