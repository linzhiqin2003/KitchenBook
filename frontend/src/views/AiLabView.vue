<script setup>
import { ref, computed, nextTick, onMounted, watch } from 'vue'
import API_BASE_URL from '../config/api'
import { AiLabSidebar, AiLabWelcome, AiLabInput, AiLabChatArea } from '../components/ailab'

// ===== ä¼šè¯ç®¡ç†çŠ¶æ€ =====
const conversations = ref([])
const currentConversationId = ref(null)
const currentConversation = ref(null)
// ç§»åŠ¨ç«¯é»˜è®¤æŠ˜å ä¾§è¾¹æ 
const isSidebarCollapsed = ref(window.innerWidth < 1024)
const isLoadingConversations = ref(false)

// ===== èŠå¤©çŠ¶æ€ =====
const isLoading = ref(false)
const inputMessage = ref('')
const messages = ref([])
const chatAreaRef = ref(null)

// ===== å›¾ç‰‡/è¯­éŸ³çŠ¶æ€ =====
const selectedImage = ref(null)
const imagePreview = ref(null)
const isOcrProcessing = ref(false)
const ocrResult = ref(null)
const fileInputRef = ref(null)
const isRecording = ref(false)
const isTranscribing = ref(false)
const recordingDuration = ref(0)
let mediaRecorder = null
let audioChunks = []
let recordingTimer = null

// ===== æµå¼çŠ¶æ€ =====
const currentReasoning = ref('')
const currentContent = ref('')
const isReasoningPhase = ref(false)
const currentStreamingIndex = ref(null)

// ===== è¯·æ±‚æ§åˆ¶ =====
let abortController = null
let currentReader = null

// ===== ç»Ÿè®¡ä¿¡æ¯ =====
const stats = ref({
  reasoningLength: 0,
  contentLength: 0,
  startTime: null,
  endTime: null
})

// ===== è®¡ç®—å±æ€§ =====
const hasMessages = computed(() => messages.value.length > 0)
const conversationTitle = computed(() => currentConversation.value?.title || '')

const STREAM_PHASE = {
  IDLE: 'idle',
  REASONING: 'reasoning',
  TOOL_CALLING: 'tool_calling',
  TOOL_EXECUTING: 'tool_executing',
  ANSWERING: 'answering',
  ERROR: 'error'
}

const TOOL_STATUS = {
  PARSING: 'parsing',
  PENDING: 'pending',
  RUNNING: 'running',
  SUCCESS: 'success',
  ERROR: 'error'
}

const createTraceId = () => `trace_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`

const formatStructuredValue = (value) => {
  if (value === null || value === undefined) return ''
  if (typeof value === 'string') return value
  try {
    return JSON.stringify(value, null, 2)
  } catch {
    return String(value)
  }
}

const safeParseJson = (raw) => {
  if (!raw || typeof raw !== 'string') return null
  try {
    return JSON.parse(raw)
  } catch {
    return null
  }
}

const normalizeToolStatus = (status) => {
  if (!status) return TOOL_STATUS.PARSING
  const normalized = String(status).toLowerCase()
  if (normalized === 'ok' || normalized === 'done') return TOOL_STATUS.SUCCESS
  if (normalized === 'failed') return TOOL_STATUS.ERROR
  if (Object.values(TOOL_STATUS).includes(normalized)) return normalized
  return TOOL_STATUS.PARSING
}

const normalizeAssistantMessage = (message) => {
  if (message?.role !== 'assistant') {
    return message
  }
  return {
    ...message,
    phase: message.phase || STREAM_PHASE.IDLE,
    subTurns: Array.isArray(message.subTurns) ? message.subTurns : [],
    currentReasoning: message.currentReasoning || '',
    currentToolCall: message.currentToolCall || null
  }
}

const normalizeMessagesForUI = (list) => {
  return (list || []).map(normalizeAssistantMessage)
}

// ===== API è°ƒç”¨ =====

// è·å–ä¼šè¯åˆ—è¡¨
const fetchConversations = async () => {
  isLoadingConversations.value = true
  try {
    const response = await fetch(`${API_BASE_URL}/api/ai/conversations/`)
    if (response.ok) {
      conversations.value = await response.json()
    }
  } catch (error) {
    console.error('è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥:', error)
  } finally {
    isLoadingConversations.value = false
  }
}

// è·å–ä¼šè¯è¯¦æƒ…ï¼ˆå«æ¶ˆæ¯ï¼‰
const fetchConversation = async (id) => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/ai/conversations/${id}/`)
    if (response.ok) {
      const data = await response.json()
      currentConversation.value = data
      messages.value = normalizeMessagesForUI(data.messages || [])
      await nextTick()
      chatAreaRef.value?.scrollToBottom()
      renderMath()
    }
  } catch (error) {
    console.error('è·å–ä¼šè¯è¯¦æƒ…å¤±è´¥:', error)
  }
}

// åˆ›å»ºæ–°ä¼šè¯
const createConversation = async () => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/ai/conversations/`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ title: 'æ–°å¯¹è¯' })
    })
    if (response.ok) {
      const data = await response.json()
      conversations.value.unshift(data)
      await selectConversation(data.id)
    }
  } catch (error) {
    console.error('åˆ›å»ºä¼šè¯å¤±è´¥:', error)
  }
}

// é€‰æ‹©ä¼šè¯
const selectConversation = async (id) => {
  currentConversationId.value = id
  await fetchConversation(id)
}

// åˆ é™¤ä¼šè¯
const deleteConversation = async (id) => {
  if (!confirm('ç¡®å®šè¦åˆ é™¤è¿™ä¸ªå¯¹è¯å—ï¼Ÿ')) return

  try {
    const response = await fetch(`${API_BASE_URL}/api/ai/conversations/${id}/`, {
      method: 'DELETE'
    })
    if (response.ok) {
      conversations.value = conversations.value.filter(c => c.id !== id)
      if (currentConversationId.value === id) {
        currentConversationId.value = null
        currentConversation.value = null
        messages.value = []
      }
    }
  } catch (error) {
    console.error('åˆ é™¤ä¼šè¯å¤±è´¥:', error)
  }
}

// ä¿å­˜æ¶ˆæ¯åˆ°åç«¯
const saveMessage = async (conversationId, role, content, reasoning = null) => {
  try {
    const response = await fetch(`${API_BASE_URL}/api/ai/conversations/${conversationId}/messages/`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ role, content, reasoning })
    })
    if (response.ok) {
      return await response.json()
    }
  } catch (error) {
    console.error('ä¿å­˜æ¶ˆæ¯å¤±è´¥:', error)
  }
  return null
}

// åˆ é™¤æ¶ˆæ¯åŠåç»­
const deleteMessageAndFollowing = async (messageId) => {
  try {
    await fetch(`${API_BASE_URL}/api/ai/messages/${messageId}/`, {
      method: 'DELETE'
    })
  } catch (error) {
    console.error('åˆ é™¤æ¶ˆæ¯å¤±è´¥:', error)
  }
}

// ===== æ¶ˆæ¯å‘é€ =====
const sendMessage = async (content = null) => {
  const text = content || inputMessage.value.trim()
  if (!text || isLoading.value) return

  // å¦‚æœæ²¡æœ‰å½“å‰ä¼šè¯ï¼Œå…ˆåˆ›å»ºä¸€ä¸ª
  if (!currentConversationId.value) {
    await createConversation()
  }

  // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
  const userMessage = { role: 'user', content: text, type: 'text' }
  messages.value.push(userMessage)
  inputMessage.value = ''

  // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°åç«¯
  const savedUserMsg = await saveMessage(currentConversationId.value, 'user', text)
  if (savedUserMsg) {
    userMessage.id = savedUserMsg.id
  }

  // åˆ·æ–°ä¼šè¯åˆ—è¡¨ä»¥æ›´æ–°æ ‡é¢˜
  fetchConversations()

  // å¼€å§‹æµå¼å“åº”
  await streamResponse()
}

// æµå¼å“åº”
const streamResponse = async () => {
  isLoading.value = true
  currentReasoning.value = ''
  currentContent.value = ''
  isReasoningPhase.value = false
  stats.value = { reasoningLength: 0, contentLength: 0, startTime: Date.now(), endTime: null }

  // æ·»åŠ ç©ºçš„ AI æ¶ˆæ¯ç”¨äºæµå¼å¡«å……
  const aiMessageIndex = messages.value.length
  currentStreamingIndex.value = aiMessageIndex
  messages.value.push({
    role: 'assistant',
    content: '',
    reasoning: '',
    type: 'text',
    isStreaming: true,
    phase: STREAM_PHASE.REASONING,
    subTurns: [],
    currentReasoning: '',
    currentToolCall: null
  })

  let streamPhase = STREAM_PHASE.REASONING
  let activeReasoning = ''
  let activeToolIndex = null
  let hasDoneEvent = false
  const streamSubTurns = []
  const streamToolCalls = new Map()

  const getAiMessage = () => messages.value[aiMessageIndex]

  const syncTraceState = () => {
    const aiMsg = getAiMessage()
    if (!aiMsg) return
    aiMsg.phase = streamPhase
    aiMsg.subTurns = streamSubTurns.map(turn => ({
      ...turn,
      toolCall: turn.toolCall ? { ...turn.toolCall } : null
    }))
    aiMsg.currentReasoning = activeReasoning
    if (activeToolIndex === null) {
      aiMsg.currentToolCall = null
      return
    }
    const activeCall = streamToolCalls.get(activeToolIndex)
    aiMsg.currentToolCall = activeCall ? { ...activeCall } : null
  }

  const pushSubTurns = () => {
    const hasReasoning = Boolean(activeReasoning.trim())
    if (!hasReasoning && streamToolCalls.size === 0) {
      return
    }

    const orderedToolCalls = Array.from(streamToolCalls.values()).sort((a, b) => a.index - b.index)
    if (orderedToolCalls.length === 0) {
      streamSubTurns.push({
        id: createTraceId(),
        reasoning: activeReasoning.trim(),
        toolCall: null
      })
    } else {
      orderedToolCalls.forEach((toolCall, index) => {
        streamSubTurns.push({
          id: createTraceId(),
          reasoning: index === 0 ? activeReasoning.trim() : '',
          toolCall: { ...toolCall }
        })
      })
    }

    activeReasoning = ''
    streamToolCalls.clear()
    activeToolIndex = null
    syncTraceState()
  }

  const setPhase = (nextPhase) => {
    if (nextPhase === streamPhase) {
      return
    }

    // å·¥å…·è°ƒç”¨ç»“æŸååˆ‡å› reasoning/answering æ—¶ï¼Œå½’æ¡£æœ¬è½® sub-turn
    if (
      (nextPhase === STREAM_PHASE.REASONING || nextPhase === STREAM_PHASE.ANSWERING || nextPhase === STREAM_PHASE.IDLE) &&
      streamToolCalls.size > 0
    ) {
      pushSubTurns()
    }

    streamPhase = nextPhase
    isReasoningPhase.value = nextPhase === STREAM_PHASE.REASONING
    syncTraceState()
  }

  const upsertToolCall = (fragment = {}, options = {}) => {
    const appendArguments = options.appendArguments !== false
    const index = Number.isInteger(fragment.index) ? fragment.index : (activeToolIndex ?? 0)
    const existing = streamToolCalls.get(index) || {
      id: fragment.id || `call_${createTraceId()}`,
      index,
      name: '',
      argumentsText: '',
      parsedArguments: null,
      status: TOOL_STATUS.PARSING,
      result: '',
      error: '',
      startedAt: Date.now(),
      finishedAt: null
    }

    if (fragment.id) existing.id = fragment.id
    if (fragment.name) existing.name = fragment.name

    if (fragment.argumentsText !== undefined) {
      const incomingArgs = typeof fragment.argumentsText === 'string'
        ? fragment.argumentsText
        : formatStructuredValue(fragment.argumentsText)
      existing.argumentsText = appendArguments ? `${existing.argumentsText}${incomingArgs}` : incomingArgs
    }

    if (fragment.status) {
      existing.status = normalizeToolStatus(fragment.status)
      if (existing.status === TOOL_STATUS.RUNNING && !existing.startedAt) {
        existing.startedAt = Date.now()
      }
      if ((existing.status === TOOL_STATUS.SUCCESS || existing.status === TOOL_STATUS.ERROR) && !existing.finishedAt) {
        existing.finishedAt = Date.now()
      }
    }

    if (fragment.result !== undefined) {
      existing.result = formatStructuredValue(fragment.result)
    }
    if (fragment.error !== undefined) {
      existing.error = formatStructuredValue(fragment.error)
      existing.status = TOOL_STATUS.ERROR
      if (!existing.finishedAt) {
        existing.finishedAt = Date.now()
      }
    }
    if (fragment.startedAt) {
      existing.startedAt = fragment.startedAt
    }
    if (fragment.finishedAt) {
      existing.finishedAt = fragment.finishedAt
    }
    if (fragment.durationMs && !existing.finishedAt) {
      existing.finishedAt = existing.startedAt + fragment.durationMs
    }

    existing.parsedArguments = safeParseJson(existing.argumentsText)
    streamToolCalls.set(index, existing)
    activeToolIndex = index
    syncTraceState()
    return existing
  }

  const ingestToolCalls = (toolCalls, options = {}) => {
    if (!Array.isArray(toolCalls)) return
    const appendArguments = options.appendArguments !== false
    const defaultStatus = options.defaultStatus || TOOL_STATUS.PARSING

    for (const call of toolCalls) {
      const callFunction = call?.function || {}
      const hasArguments = callFunction.arguments !== undefined || call.arguments !== undefined || call.args !== undefined
      upsertToolCall({
        index: call?.index,
        id: call?.id,
        name: callFunction.name || call?.name,
        argumentsText: hasArguments ? (callFunction.arguments ?? call.arguments ?? call.args) : undefined,
        status: call?.status || defaultStatus,
        result: call?.result,
        error: call?.error,
        durationMs: call?.duration_ms || call?.durationMs
      }, { appendArguments })
    }
  }

  const appendReasoningChunk = (chunk) => {
    if (!chunk) return
    setPhase(STREAM_PHASE.REASONING)
    currentReasoning.value += chunk
    activeReasoning += chunk
    const aiMsg = getAiMessage()
    if (aiMsg) {
      aiMsg.reasoning = currentReasoning.value
    }
    syncTraceState()
    chatAreaRef.value?.scrollToBottom()
  }

  const appendContentChunk = (chunk) => {
    if (!chunk) return
    setPhase(STREAM_PHASE.ANSWERING)
    currentContent.value += chunk
    const aiMsg = getAiMessage()
    if (aiMsg) {
      aiMsg.content = currentContent.value
    }
    syncTraceState()
    chatAreaRef.value?.scrollToBottom()
    renderMath()
  }

  const markToolCallsStatus = (status) => {
    for (const call of streamToolCalls.values()) {
      upsertToolCall({
        index: call.index,
        status
      }, { appendArguments: false })
    }
  }

  const handleOpenAIChunk = (payload) => {
    const delta = payload?.delta || payload?.choices?.[0]?.delta || payload?.chunk?.choices?.[0]?.delta
    const finishReason = payload?.finish_reason ?? payload?.choices?.[0]?.finish_reason ?? payload?.chunk?.choices?.[0]?.finish_reason

    if (typeof delta?.reasoning_content === 'string' && delta.reasoning_content) {
      appendReasoningChunk(delta.reasoning_content)
    }

    if (Array.isArray(delta?.tool_calls) && delta.tool_calls.length > 0) {
      setPhase(STREAM_PHASE.TOOL_CALLING)
      ingestToolCalls(delta.tool_calls, { appendArguments: true, defaultStatus: TOOL_STATUS.PARSING })
    }

    if (typeof delta?.content === 'string' && delta.content) {
      appendContentChunk(delta.content)
    }

    if (finishReason === 'tool_calls') {
      markToolCallsStatus(TOOL_STATUS.PENDING)
    }
    if (finishReason === 'stop') {
      hasDoneEvent = true
      stats.value.endTime = Date.now()
      setPhase(STREAM_PHASE.IDLE)
    }
  }

  const handleToolEvent = (payload, eventType) => {
    const rawToolCall = payload.tool_call || payload.call || payload
    const hasArguments = rawToolCall?.arguments !== undefined || rawToolCall?.args !== undefined || rawToolCall?.function?.arguments !== undefined

    if (eventType === 'tool_execution_start') {
      setPhase(STREAM_PHASE.TOOL_EXECUTING)
    } else {
      setPhase(STREAM_PHASE.TOOL_CALLING)
    }

    upsertToolCall({
      index: rawToolCall?.index,
      id: rawToolCall?.id || payload.tool_call_id,
      name: rawToolCall?.function?.name || rawToolCall?.name || payload.name,
      argumentsText: hasArguments
        ? (rawToolCall?.function?.arguments ?? rawToolCall?.arguments ?? rawToolCall?.args)
        : undefined,
      status: payload.status || (
        eventType === 'tool_execution_start'
          ? TOOL_STATUS.RUNNING
          : eventType === 'tool_execution_error'
            ? TOOL_STATUS.ERROR
            : eventType === 'tool_execution_result' || eventType === 'tool_result'
              ? TOOL_STATUS.SUCCESS
              : TOOL_STATUS.PARSING
      ),
      result: payload.result ?? payload.output,
      error: payload.error,
      durationMs: payload.duration_ms || payload.durationMs
    }, { appendArguments: eventType !== 'tool_call' })
  }

  const handleStreamEvent = (parsed) => {
    if (!parsed || typeof parsed !== 'object') return

    switch (parsed.type) {
      case 'reasoning_start':
        setPhase(STREAM_PHASE.REASONING)
        break

      case 'reasoning':
        appendReasoningChunk(parsed.content)
        break

      case 'reasoning_end':
        isReasoningPhase.value = false
        break

      case 'tool_call':
      case 'tool_call_start':
      case 'tool_call_delta':
      case 'tool_call_end':
      case 'tool_execution_start':
      case 'tool_execution_result':
      case 'tool_execution_error':
      case 'tool_result':
        handleToolEvent(parsed, parsed.type)
        break

      case 'content_start':
        setPhase(STREAM_PHASE.ANSWERING)
        break

      case 'content':
        appendContentChunk(parsed.content)
        break

      case 'done':
        hasDoneEvent = true
        stats.value.reasoningLength = parsed.reasoning_length ?? currentReasoning.value.length
        stats.value.contentLength = parsed.content_length ?? currentContent.value.length
        stats.value.endTime = Date.now()
        if (parsed.finish_reason === 'tool_calls') {
          markToolCallsStatus(TOOL_STATUS.PENDING)
        }
        setPhase(STREAM_PHASE.IDLE)
        break

      case 'error':
        throw new Error(parsed.error)

      default:
        if (parsed.phase && Object.values(STREAM_PHASE).includes(parsed.phase)) {
          setPhase(parsed.phase)
        }
        if (Array.isArray(parsed.tool_calls) && parsed.tool_calls.length > 0) {
          setPhase(STREAM_PHASE.TOOL_CALLING)
          ingestToolCalls(parsed.tool_calls, { appendArguments: parsed.type === 'tool_calls_delta', defaultStatus: TOOL_STATUS.PARSING })
        }
        if (parsed.delta || parsed.choices || parsed.chunk) {
          handleOpenAIChunk(parsed)
        }
    }
  }

  // æ„å»º API æ¶ˆæ¯
  const apiMessages = messages.value
    .filter(m => m.type === 'text' && (m.role === 'user' || m.role === 'assistant') && !m.isStreaming)
    .map(m => ({
      role: m.role,
      content: m.content?.replace(/\n\n\*\[å·²åœæ­¢ç”Ÿæˆ\]\*$/, '') || ''
    }))
    .filter(m => m.content)

  abortController = new AbortController()

  try {
    const response = await fetch(`${API_BASE_URL}/api/ai/speciale/`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ messages: apiMessages }),
      signal: abortController.signal
    })

    if (!response.ok) {
      const error = await response.json()
      throw new Error(error.error || 'è¯·æ±‚å¤±è´¥')
    }

    const reader = response.body.getReader()
    currentReader = reader
    const decoder = new TextDecoder()
    let buffer = ''

    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      if (!isLoading.value) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        if (!line || line.startsWith(':')) continue

        if (line.startsWith('data: ')) {
          const data = line.slice(6).trim()
          if (data === '[DONE]') break
          if (!data) continue

          try {
            const parsed = JSON.parse(data)
            handleStreamEvent(parsed)
          } catch (e) {
            if (!(e instanceof SyntaxError)) {
              throw e
            }
          }
        }
      }
    }

    if (!hasDoneEvent) {
      stats.value.reasoningLength = currentReasoning.value.length
      stats.value.contentLength = currentContent.value.length
      stats.value.endTime = Date.now()
    }

    if (activeReasoning.trim() || streamToolCalls.size > 0) {
      pushSubTurns()
    }

    setPhase(STREAM_PHASE.IDLE)
    const aiMsg = getAiMessage()
    if (aiMsg) {
      aiMsg.isStreaming = false
      aiMsg.phase = STREAM_PHASE.IDLE
      aiMsg.currentReasoning = ''
      aiMsg.currentToolCall = null
      aiMsg.stats = { ...stats.value }
    }

    // ä¿å­˜ AI æ¶ˆæ¯åˆ°åç«¯
    const savedAiMsg = await saveMessage(
      currentConversationId.value,
      'assistant',
      currentContent.value,
      currentReasoning.value
    )
    if (savedAiMsg) {
      messages.value[aiMessageIndex].id = savedAiMsg.id
    }

    renderMath()

  } catch (error) {
    if (error.name === 'AbortError') {
      return
    }
    const aiMsg = getAiMessage()
    if (aiMsg) {
      aiMsg.content = `æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€ç‚¹é—®é¢˜ ğŸ˜…\n\n${error.message}\n\nè¯·ç¨åå†è¯•~`
      aiMsg.reasoning = currentReasoning.value
      aiMsg.phase = STREAM_PHASE.ERROR
      aiMsg.isStreaming = false
      aiMsg.currentReasoning = ''
      aiMsg.currentToolCall = null
      aiMsg.stats = {
        ...stats.value,
        endTime: Date.now(),
        reasoningLength: currentReasoning.value.length,
        contentLength: currentContent.value.length
      }
    }
  } finally {
    isLoading.value = false
    isReasoningPhase.value = false
    abortController = null
    currentReader = null
    currentStreamingIndex.value = null
    chatAreaRef.value?.scrollToBottom()
  }
}

// åœæ­¢ç”Ÿæˆ
const stopGeneration = async () => {
  if (abortController) {
    abortController.abort()
  }
  if (currentReader) {
    try {
      await currentReader.cancel()
    } catch (e) {
      // å¿½ç•¥å–æ¶ˆé”™è¯¯
    }
  }

  if (currentStreamingIndex.value !== null && messages.value[currentStreamingIndex.value]) {
    const msg = messages.value[currentStreamingIndex.value]
    msg.isStreaming = false
    msg.stopped = true
    if (msg.content) {
      msg.content += '\n\n*[å·²åœæ­¢ç”Ÿæˆ]*'
    } else if (msg.reasoning) {
      msg.content = '*[å·²åœæ­¢ç”Ÿæˆ]*'
    }
    msg.stats = {
      ...stats.value,
      endTime: Date.now(),
      reasoningLength: currentReasoning.value.length,
      contentLength: currentContent.value.length
    }
  }

  isLoading.value = false
  isReasoningPhase.value = false
  abortController = null
  currentReader = null
  currentStreamingIndex.value = null
}

// ç¼–è¾‘æ¶ˆæ¯
const handleEditMessage = async (messageId, newContent, index) => {
  if (!messageId || !newContent) return

  // åˆ é™¤è¯¥æ¶ˆæ¯åŠåç»­
  await deleteMessageAndFollowing(messageId)

  // ä»æœ¬åœ°ç§»é™¤è¯¥æ¶ˆæ¯åŠåç»­
  messages.value = messages.value.slice(0, index)

  // é‡æ–°å‘é€
  await sendMessage(newContent)
}

// é‡æ–°ç”Ÿæˆ
const handleRegenerate = async (messageId, index) => {
  if (!messageId) return

  // æ‰¾åˆ°å¯¹åº”çš„ç”¨æˆ·æ¶ˆæ¯
  let userMessageIndex = index - 1
  while (userMessageIndex >= 0 && messages.value[userMessageIndex].role !== 'user') {
    userMessageIndex--
  }

  if (userMessageIndex < 0) return

  const userContent = messages.value[userMessageIndex].content

  // åˆ é™¤ AI æ¶ˆæ¯
  await deleteMessageAndFollowing(messageId)

  // ä»æœ¬åœ°ç§»é™¤è¯¥æ¶ˆæ¯åŠåç»­
  messages.value = messages.value.slice(0, index)

  // é‡æ–°ç”Ÿæˆ
  await streamResponse()
}

// ===== å›¾ç‰‡å¤„ç† =====
const triggerFileInput = () => {
  fileInputRef.value?.click()
}

const handleFileSelect = (event) => {
  const file = event.target.files?.[0]
  if (!file) return

  if (!file.type.startsWith('image/')) {
    alert('è¯·é€‰æ‹©å›¾ç‰‡æ–‡ä»¶')
    return
  }

  if (file.size > 10 * 1024 * 1024) {
    alert('å›¾ç‰‡å¤§å°ä¸èƒ½è¶…è¿‡ 10MB')
    return
  }

  selectedImage.value = file

  const reader = new FileReader()
  reader.onload = (e) => {
    imagePreview.value = e.target.result
  }
  reader.readAsDataURL(file)
  ocrResult.value = null
}

const removeImage = () => {
  selectedImage.value = null
  imagePreview.value = null
  ocrResult.value = null
  if (fileInputRef.value) {
    fileInputRef.value.value = ''
  }
}

const processOCR = async () => {
  if (!selectedImage.value || isOcrProcessing.value) return

  isOcrProcessing.value = true
  ocrResult.value = null

  try {
    const formData = new FormData()
    formData.append('image', selectedImage.value)

    const response = await fetch(`${API_BASE_URL}/api/ai/ocr/`, {
      method: 'POST',
      body: formData
    })

    const data = await response.json()

    if (!response.ok) {
      throw new Error(data.error || 'OCR è¯†åˆ«å¤±è´¥')
    }

    ocrResult.value = data.markdown
    inputMessage.value = `è¯·è§£ç­”ä»¥ä¸‹é¢˜ç›®ï¼š\n\n${data.markdown}`

  } catch (error) {
    alert(`OCR è¯†åˆ«å¤±è´¥: ${error.message}`)
  } finally {
    isOcrProcessing.value = false
  }
}

const handlePaste = (event) => {
  const items = event.clipboardData?.items
  if (!items) return

  for (const item of items) {
    if (item.type.startsWith('image/')) {
      event.preventDefault()
      const file = item.getAsFile()
      if (file) {
        selectedImage.value = file
        const reader = new FileReader()
        reader.onload = (e) => {
          imagePreview.value = e.target.result
        }
        reader.readAsDataURL(file)
        ocrResult.value = null
      }
      break
    }
  }
}

const handleSend = async () => {
  if (selectedImage.value && !ocrResult.value) {
    await processOCR()
    if (!ocrResult.value) return
  }
  await sendMessage()
  removeImage()
}

// ===== è¯­éŸ³å½•åˆ¶ =====
const toggleRecording = async () => {
  if (isRecording.value) {
    stopRecording()
  } else {
    await startRecording()
  }
}

const startRecording = async () => {
  try {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      if (location.protocol === 'http:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
        alert('è¯­éŸ³è¾“å…¥éœ€è¦ HTTPS å®‰å…¨è¿æ¥ã€‚')
      } else {
        alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³å½•åˆ¶åŠŸèƒ½ã€‚')
      }
      return
    }

    const stream = await navigator.mediaDevices.getUserMedia({
      audio: { channelCount: 1, sampleRate: 16000 }
    })

    const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
      ? 'audio/webm;codecs=opus'
      : 'audio/webm'

    mediaRecorder = new MediaRecorder(stream, { mimeType })
    audioChunks = []
    recordingDuration.value = 0

    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.push(event.data)
      }
    }

    mediaRecorder.onstop = async () => {
      stream.getTracks().forEach(track => track.stop())
      const audioBlob = new Blob(audioChunks, { type: mimeType })
      const duration = recordingDuration.value
      await transcribeAudio(audioBlob, duration)
    }

    recordingTimer = setInterval(() => {
      recordingDuration.value++
      if (recordingDuration.value >= 60) {
        stopRecording()
      }
    }, 1000)

    mediaRecorder.start(1000)
    isRecording.value = true

  } catch (error) {
    console.error('å½•éŸ³å¤±è´¥:', error)
    if (error.name === 'NotAllowedError') {
      alert('è¯·å…è®¸éº¦å…‹é£è®¿é—®æƒé™')
    } else if (error.name === 'NotFoundError') {
      alert('æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡')
    } else {
      alert('å½•éŸ³åˆå§‹åŒ–å¤±è´¥: ' + error.message)
    }
  }
}

const stopRecording = () => {
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    mediaRecorder.stop()
  }
  if (recordingTimer) {
    clearInterval(recordingTimer)
    recordingTimer = null
  }
  isRecording.value = false
}

const transcribeAudio = async (audioBlob, duration = 0) => {
  isTranscribing.value = true

  try {
    const formData = new FormData()
    formData.append('audio', audioBlob, 'recording.webm')
    formData.append('duration', duration.toString())

    const response = await fetch(`${API_BASE_URL}/api/ai/transcribe/`, {
      method: 'POST',
      body: formData
    })

    const data = await response.json()

    if (!response.ok) {
      throw new Error(data.error || 'è½¬å½•å¤±è´¥')
    }

    if (data.text) {
      inputMessage.value = (inputMessage.value ? inputMessage.value + ' ' : '') + data.text
    }

  } catch (error) {
    console.error('è½¬å½•å¤±è´¥:', error)
    alert('è¯­éŸ³è½¬å½•å¤±è´¥: ' + error.message)
  } finally {
    isTranscribing.value = false
    recordingDuration.value = 0
  }
}

// ===== MathJax æ¸²æŸ“ =====
const renderMath = async () => {
  await nextTick()
  if (window.MathJax) {
    window.MathJax.typesetPromise?.()
  }
}

// ===== ä¾§è¾¹æ æ§åˆ¶ =====
const toggleSidebar = () => {
  isSidebarCollapsed.value = !isSidebarCollapsed.value
}

// ===== å¿«æ·é—®é¢˜ =====
const handleQuickAsk = (prompt) => {
  inputMessage.value = prompt
  sendMessage()
}

// ===== åˆå§‹åŒ– =====
onMounted(async () => {
  // åŠ è½½ MathJax
  if (!window.MathJax) {
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
      },
      svg: { fontCache: 'global' },
      startup: {
        ready: () => {
          window.MathJax.startup.defaultReady()
        }
      }
    }
    const script = document.createElement('script')
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js'
    script.async = true
    document.head.appendChild(script)
  }

  // è·å–ä¼šè¯åˆ—è¡¨
  await fetchConversations()

  // å¦‚æœæœ‰ä¼šè¯ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
  if (conversations.value.length > 0) {
    await selectConversation(conversations.value[0].id)
  }

  setTimeout(renderMath, 500)
})
</script>

<template>
  <div class="h-dvh w-full fixed inset-0 bg-gradient-to-br from-slate-50 via-white to-violet-50/30 flex overflow-hidden">
    <!-- ä¾§è¾¹æ  -->
    <AiLabSidebar
      :conversations="conversations"
      :current-id="currentConversationId"
      :is-collapsed="isSidebarCollapsed"
      @select="selectConversation"
      @new="createConversation"
      @delete="deleteConversation"
      @toggle-collapse="toggleSidebar"
    />

    <!-- ä¸»å†…å®¹åŒº -->
    <div class="flex-1 flex flex-col min-w-0">
      <!-- ç§»åŠ¨ç«¯é¡¶éƒ¨æ  -->
      <header class="shrink-0 h-14 bg-white/80 backdrop-blur-sm border-b border-gray-200 flex items-center px-4 gap-3 lg:hidden">
        <button
          @click="toggleSidebar"
          class="w-9 h-9 rounded-lg hover:bg-gray-100 flex items-center justify-center transition-colors cursor-pointer"
        >
          <svg class="w-5 h-5 text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/>
          </svg>
        </button>

        <div class="flex items-center gap-2 flex-1 min-w-0">
          <div class="w-8 h-8 rounded-lg bg-gradient-to-br from-violet-500 to-purple-600 flex items-center justify-center shadow-md shrink-0">
            <span class="text-sm">âœ¨</span>
          </div>
          <div class="min-w-0">
            <h1 class="text-sm font-semibold text-gray-800 truncate">AI Lab</h1>
          </div>
        </div>

        <router-link
          to="/ai-lab/studio"
          class="w-9 h-9 rounded-lg bg-violet-50 hover:bg-violet-100 flex items-center justify-center transition-colors"
          title="AI Studio"
        >
          <span class="text-sm">ğŸ™ï¸</span>
        </router-link>
      </header>

      <!-- æ¬¢è¿å± / èŠå¤©åŒºåŸŸ -->
      <AiLabWelcome
        v-if="!hasMessages"
        :is-loading="isLoading"
        @ask="handleQuickAsk"
      />

      <AiLabChatArea
        v-else
        ref="chatAreaRef"
        :messages="messages"
        :current-streaming-index="currentStreamingIndex"
        :is-reasoning-phase="isReasoningPhase"
        :conversation-title="conversationTitle"
        @edit="handleEditMessage"
        @regenerate="handleRegenerate"
      />

      <!-- å›¾ç‰‡é¢„è§ˆ -->
      <Transition name="fade">
        <div v-if="imagePreview" class="shrink-0 px-4 pb-2">
          <div class="max-w-4xl mx-auto p-3 bg-gray-50 rounded-xl border border-gray-200">
            <div class="flex items-start gap-3">
              <div class="relative shrink-0">
                <img
                  :src="imagePreview"
                  alt="é¢„è§ˆ"
                  class="w-20 h-20 object-cover rounded-lg border border-gray-200"
                />
                <button
                  @click="removeImage"
                  class="absolute -top-2 -right-2 w-6 h-6 bg-red-500 hover:bg-red-600 text-white rounded-full flex items-center justify-center shadow-md cursor-pointer"
                >
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/>
                  </svg>
                </button>
              </div>
              <div class="flex-1 min-w-0">
                <div class="text-sm text-gray-600 mb-2 truncate">{{ selectedImage?.name }}</div>
                <div v-if="ocrResult" class="text-xs text-green-600 flex items-center gap-1">
                  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"/>
                  </svg>
                  å·²è¯†åˆ«
                </div>
                <button
                  v-else
                  @click="processOCR"
                  :disabled="isOcrProcessing"
                  class="px-3 py-1.5 text-xs bg-violet-600 hover:bg-violet-500 text-white rounded-lg disabled:bg-gray-300 cursor-pointer flex items-center gap-1.5"
                >
                  <svg v-if="isOcrProcessing" class="w-3.5 h-3.5 animate-spin" fill="none" viewBox="0 0 24 24">
                    <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                    <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"></path>
                  </svg>
                  {{ isOcrProcessing ? 'è¯†åˆ«ä¸­...' : 'è¯†åˆ«å›¾ç‰‡' }}
                </button>
              </div>
            </div>
          </div>
        </div>
      </Transition>

      <!-- éšè—çš„æ–‡ä»¶è¾“å…¥ -->
      <input
        ref="fileInputRef"
        type="file"
        accept="image/*"
        class="hidden"
        @change="handleFileSelect"
      />

      <!-- è¾“å…¥åŒºåŸŸï¼ˆæœ‰æ¶ˆæ¯æ—¶æ˜¾ç¤ºï¼‰ -->
      <AiLabInput
        v-if="hasMessages"
        v-model="inputMessage"
        :is-loading="isLoading"
        :is-recording="isRecording"
        :is-transcribing="isTranscribing"
        :is-ocr-processing="isOcrProcessing"
        :recording-duration="recordingDuration"
        :has-image="!!selectedImage"
        @send="handleSend"
        @stop="stopGeneration"
        @image-click="triggerFileInput"
        @voice-click="toggleRecording"
        @paste="handlePaste"
      />
    </div>

    <!-- ç§»åŠ¨ç«¯ä¾§è¾¹æ é®ç½© -->
    <Transition name="fade">
      <div
        v-if="!isSidebarCollapsed"
        class="fixed inset-0 bg-black/50 z-40 lg:hidden"
        @click="isSidebarCollapsed = true"
      ></div>
    </Transition>
  </div>
</template>

<style scoped>
/* ä¸»é¢˜å˜é‡ */
:root {
  --theme-50: #f5f3ff;
  --theme-100: #ede9fe;
  --theme-200: #ddd6fe;
  --theme-300: #c4b5fd;
  --theme-400: #a78bfa;
  --theme-500: #8b5cf6;
  --theme-600: #7c3aed;
  --theme-700: #6d28d9;
  --theme-gradient: linear-gradient(135deg, #8b5cf6, #d946ef);
  --theme-gradient-btn: linear-gradient(to right, #8b5cf6, #9333ea);
  --theme-shadow: rgba(139, 92, 246, 0.2);
}

/* åŠ¨æ€è§†å£é«˜åº¦å…¼å®¹ */
.h-dvh {
  height: 100vh;
  height: 100dvh;
}

/* æ·¡å…¥æ·¡å‡ºåŠ¨ç”» */
.fade-enter-active,
.fade-leave-active {
  transition: opacity 0.2s ease;
}
.fade-enter-from,
.fade-leave-to {
  opacity: 0;
}

/* å…¨å±€æ»šåŠ¨æ¡ */
::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}

::-webkit-scrollbar-track {
  background: transparent;
}

::-webkit-scrollbar-thumb {
  background: #d1d5db;
  border-radius: 3px;
}

::-webkit-scrollbar-thumb:hover {
  background: #9ca3af;
}

/* éšè—æ»šåŠ¨æ¡ä½†ä¿æŒåŠŸèƒ½ */
.scrollbar-hide {
  -ms-overflow-style: none;
  scrollbar-width: none;
}

.scrollbar-hide::-webkit-scrollbar {
  display: none;
}
</style>
